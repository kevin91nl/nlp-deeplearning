{
 "metadata": {
  "name": "",
  "signature": "sha256:966cdfc291ba933c134cdaf670768017c4c9fc82ca9df8456bd22c4d7813b598"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Entity Salience baseline\n",
      "\n",
      "The goal of this notebook is to replicate the Entity Salience baseline as described in https://www.cs.cmu.edu/~jdunietz/publications/salience.pdf. This notebook is provided by Kevin Jacobs (https://www.data-blogger.com/).\n",
      "\n",
      "## New York Times corpus\n",
      "The authors of the paper only use a subset of the New York Times corpus and only use articles from 2003-2007 and by filtering out articles and summaries that were very short or very long, as well as several special article types (e.g., corrections and letters to the editor). The full labeled dataset includes 110,639 documents with 2,229,728 labeled entities; about 14% are marked as salient. 9,719 documents from 2007 are used as test data and the rest as training.\n",
      "\n",
      "## New York Times annotations\n",
      "The New York Times corpus with annotations of salient entities is found on https://github.com/google-research-datasets/nyt-salience.\n",
      "\n",
      "## Baseline features\n",
      "\n",
      "The paper described several baseline features. In the next table, these features are summarized. The first part of the table described independent features. The second part of the table describes an approach in which several methods are added to a previous approach. You can see that the score gets higher further down the second part of the table. The table describes the precision ($P$), recall ($R$) and $F_1$ score of each approach.\n",
      "\n",
      "**Feature name** | **Description** | **$P$** | **$R$** | **$F_1$**\n",
      "---|---|---|---|---\n",
      "* Positional baseline * | Binary feature indicating that an entity is mentioned in the first sentence. | 59.5 | 37.8 | 46.2\n",
      "*head-count* | Number of times the head word of the entity's first mention appears. | 37.3 | 54.7 | 44.4\n",
      "*mentions* | Conjunction of the number of named (*Barack Obama*), nominal (*president*), pronominal (*he*), and total mentions of the entity. | 57.2 | 51.3 | 54.1\n",
      "**Feature name** | **Description** | **$P$** | **$R$** | **$F_1$**\n",
      "*1st-loc* | Index of the sentence in which the first mention of the entity appears. | 46.1 | 60.2 | 52.2\n",
      "*+head-count* | Number of times the head word of the entity's first mention appears. | 52.6 | 63.4 | 57.5\n",
      "*+mentions* | Conjunction of the number of named (*Barack Obama*), nominal (*president*), pronominal (*he*), and total mentions of the entity. | 59.3 | 61.3 | 60.3\n",
      "*+headline* | POS tag of each word that appears in the least one mention and also in the headline. | 59.1 | 61.9 | 60.5\n",
      "*+head-lex* | Lowercased word of the first mention. | 59.7 | 63.6 | 61.6\n",
      "*+centrality* | The method described in the paper. | 60.5 | 63.5 | 62.0\n",
      "\n",
      "## Loading the data\n",
      "\n",
      "The first step is to load the data and to align it with the annotations such that it is inline with the paper.\n",
      "\n",
      "### Check the folders\n",
      "\n",
      "In the next section, the folders should be specified. Afterwards, it is checked whether the paths are pointing to the correct folders."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "# Define the path to the New York Times corpus (a path to a folder consisting of folders per year)\n",
      "nyt_folder = '/media/sf_Ubuntu/Thesis/data/nyt'\n",
      "# The path to the New York Salience annotation files (a folder containing nyt_train and nyt_eval)\n",
      "nyt_salience_folder = '/media/sf_Ubuntu/Thesis/data/nyt-salience'\n",
      "\n",
      "# The years used in the paper\n",
      "years = list(range(2003, 2007 + 1))\n",
      "\n",
      "# Check whether these folders exist\n",
      "for year in years:\n",
      "    year_path = os.path.join(nyt_folder, str(year))\n",
      "    assert os.path.exists(year_path), 'Path not found: %s' % year_path\n",
      "    \n",
      "# The annotated train and test files\n",
      "nyt_train = os.path.join(nyt_salience_folder, 'nyt-train')\n",
      "nyt_test = os.path.join(nyt_salience_folder, 'nyt-eval')\n",
      "\n",
      "# Check whether these files exist\n",
      "assert os.path.exists(nyt_train), 'The annotated train file does not exist: %s' % nyt_train\n",
      "assert os.path.exists(nyt_test), 'The annotated test file does not exist: %s' % nyt_test\n",
      "\n",
      "print('All files are in place!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "All files are in place!\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load files into memory\n",
      "\n",
      "The next step is to load the files into memory such that we can work easily with it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# My favorite data-loading library\n",
      "import pandas as pd\n",
      "\n",
      "def load_annotation(path):\n",
      "    \"\"\"\n",
      "    Loads an annotation file (nyt_train, nyt_eval) into memory.\n",
      "    \"\"\"\n",
      "    doc_id = None\n",
      "    \n",
      "    # Document dataframe columns and rows initialization\n",
      "    df_docs_columns = ['doc_id', 'title']\n",
      "    df_docs_rows = []\n",
      "    \n",
      "    # Entity dataframe columns and rows initialization\n",
      "    df_entities_columns = ['doc_id', 'e_id', 'is_salient', 'mention_count', 'first_mention', 'byte_offset_start_position_first_mention', 'byte_offset_end_position_first_mention', 'MID']\n",
      "    df_entities_rows = []\n",
      "    \n",
      "    with open(path, 'rb') as input_file:\n",
      "        lines = input_file.readlines()\n",
      "        \n",
      "    for line_number, line in enumerate(lines):\n",
      "        # Clean the line\n",
      "        line = line.strip()\n",
      "        # Find the fields in the line\n",
      "        fields = line.split(\"\\t\")\n",
      "        # There are two types of lines: those containing entities and lines containing document descriptors\n",
      "        if len(fields) == 2:\n",
      "            # It's a document descriptor\n",
      "            df_docs_rows.append(fields)\n",
      "            doc_id = fields[0]\n",
      "        elif len(fields) == 7:\n",
      "            # It's a entity descriptor\n",
      "            df_entities_rows.append([doc_id] + fields)\n",
      "        elif len(fields) == 1:\n",
      "            # It's an empty line\n",
      "            pass\n",
      "        else:\n",
      "            # Something weird happened\n",
      "            raise Exception(\"Unknown line type detected: %s (%d field(s))\" % (line, len(fields)))\n",
      "    \n",
      "    # Create the document dataframe\n",
      "    df_docs = pd.DataFrame(df_docs_rows, columns=df_docs_columns)\n",
      "    df_docs = df_docs.set_index('doc_id')\n",
      "    \n",
      "    # Create the entity dataframe\n",
      "    df_entities = pd.DataFrame(df_entities_rows, columns=df_entities_columns)\n",
      "    df_entities = df_entities.set_index(['doc_id', 'e_id'])\n",
      "    \n",
      "    return df_docs, df_entities\n",
      "        \n",
      "print('Loading %s...' % nyt_train)\n",
      "df_train_docs, df_train_entities = load_annotation(nyt_train)\n",
      "print('Loading %s...' % nyt_test)\n",
      "df_test_docs, df_test_entities = load_annotation(nyt_test)\n",
      "print('The NYT annotations are loaded.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'nyt_test' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-57-78c870f6ff0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mdf_train_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnyt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mdf_test_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnyt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The NYT annotations are loaded.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'nyt_test' is not defined"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}